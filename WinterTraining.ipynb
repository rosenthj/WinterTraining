{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winter Training Script\n",
    "\n",
    "This is a training script for the evaluation function of the Winter chess engine.\n",
    "It is only slightly modified compared to what I have been using in internal development.\n",
    "\n",
    "The script relies on Numpy, Pandas, Tensorflow 2.0 and tensorflow_probability. You should use a Python 3 kernel.\n",
    "\n",
    "I will try to keep this script roughly up to date with the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "colab_type": "code",
    "id": "Fg008rAFEn0W",
    "outputId": "c5cf4a8c-ca33-435f-948c-161d471c0721"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3Y9dbtQYyYz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "In the following cells we load and preprocess our training data.\n",
    "\n",
    "Training data should be stored with Pandas as an array where the first two collumns consist of the win and the win+draw collumns and the remaining collumns consist of the position features.\n",
    "\n",
    "In order generate such matrices, you can use pgn-extract to transform PGNs into a format that Winter can understand. Then you may feed Winter the generated files which will create a .csv file. Finally you can use pandas to transform the .csv to a more compact format to use with this script.\n",
    "\n",
    "The pipeline to generate and integrate a new neural net in Winter is roughly as follows:\n",
    "\n",
    "1. Get and compile the latest [pgn-extract](https://www.cs.kent.ac.uk/people/staff/djb/pgn-extract/) by David J. Barnes.\n",
    "2. Use pgn-extract on your .pgn file with the arguments `-Wuci` and `--notags`. This will create a file readable by Winter.\n",
    "3. Run Winter from command line. Call `gen_eval_csv filename out_filename` where filename is the name of the file generated in 2. and out_filename is what Winter should call the generated file. This will create a .csv dataset file (described below) based on pseudo-quiescent positions from the input games.\n",
    "4. Either transform the .csv to a pandas format or modify this script to read out the information in the format you prefer.\n",
    "5. Run this training script\n",
    "6. Copy the output of the last cell and use it as a replacement for the contents of Winter's src/net_weights.h file\n",
    "7. Recompile Winter with the new weights.\n",
    "\n",
    "Depending on what you are doing (eg. modifying the architecture) you may need to do some more work than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Y5QN59PXVvu"
   },
   "outputs": [],
   "source": [
    "def np_to_tensor(val):\n",
    "  val_np = np.asarray(val, np.int8)\n",
    "  return tf.convert_to_tensor(val_np, tf.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the next cell to match your input data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErDgjIJuOpeA"
   },
   "outputs": [],
   "source": [
    "X = np_to_tensor(pd.read_pickle('./nn_inputsetv26.bz2', compression='bz2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything correctly, the first two collumns should be the win and win+draw probabilities respectively. These in turn imply W/D/L probabilities and are currently the output of the model that Winter expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "543rnwj9QK7d"
   },
   "outputs": [],
   "source": [
    "Y = X[:,0:2]\n",
    "Y = tf.cast(Y, tf.float32)\n",
    "X = X[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale parameter is used to rescale input features in the desired way. This is integrated into the weights at the end in order to save computational overhead and code complexity.\n",
    "\n",
    "While the function used here is likely not optimal, I have played around a bit with it and not found anything better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "aWhsmHrllmVi",
    "outputId": "d793d138-7f96-4b1e-b22a-b3d8938f5a93"
   },
   "outputs": [],
   "source": [
    "scale = 1 / tf.math.maximum(tf.cast(tf.reduce_max(X, axis=0), tf.float32), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the default scale function, then the inverse gives us and understanding of the expected range of values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWKGMvigrn9B"
   },
   "outputs": [],
   "source": [
    "1 / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set things up identically to how I do, then the very first sample is the standard starting position. I think this is a simple base case to check to make sure features make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9ZxylxmmXvPP",
    "outputId": "e11d7214-d5a0-409f-e600-41a40ee7135d"
   },
   "outputs": [],
   "source": [
    "X[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tZnAfJ1SLht"
   },
   "outputs": [],
   "source": [
    "num_test = 20000\n",
    "\n",
    "X_train = X[num_test:,:]\n",
    "X_test = X[:num_test,:]\n",
    "\n",
    "Y_train = Y[num_test:]\n",
    "#Y_train = Y_train[..., tf.newaxis]\n",
    "Y_test = Y[:num_test]\n",
    "#Y_test = Y_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gk5o4HMIRxxB",
    "outputId": "1aa5faf0-b3f9-4293-9bde-45479915b7f7"
   },
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaSI_Bg2Tx9U"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, Y_train)).shuffle(10000).batch(128)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Notes on Model Choice\n",
    "\n",
    "The next two cells are used to define the model.\n",
    "\n",
    "#### Some Things to Keep in Mind\n",
    "\n",
    "Winter does not itself rely on Tensorflow or any other ML library. This means that any feature that is reflected in the final model must get supported in the Winter codebase. Should you need support for some feature, please raise an issue on github or contact me personally. Unfortunately I cannot guarantee support for arbitrary functions.\n",
    "\n",
    "Winter is designed for play on CPU. Attempting to train large models with batching on GPU in mind is not wise for this reason as I do not have access to a GPU at all for this project. I would be willing to help support a GPU branch of Winter, but for this reason I cannot be the primary contributor to such a feature.\n",
    "\n",
    "The last cell in this script generates a source file that can be integrated into Winter. Longterm I would like to support ONNX and loading external models, but for now I would like to stick to small models that can be backed into Winter's source directly. Modifying the model definition in the following cells may also require changing the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoMCjWUYkIDO"
   },
   "outputs": [],
   "source": [
    "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "       layer: tensorflow keras layers (with kernel attribute)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super(SpectralNormalization, self).__init__(layer, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build `Layer`\"\"\"\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "\n",
    "            if not hasattr(self.layer, 'kernel'):\n",
    "                raise ValueError(\n",
    "                    '`SpectralNormalization` must wrap a layer that'\n",
    "                    ' contains a `kernel` for weights')\n",
    "\n",
    "            self.w = self.layer.kernel\n",
    "            self.w_shape = self.w.shape.as_list()\n",
    "            self.u = self.add_variable(\n",
    "                shape=tuple([1, self.w_shape[-1]]),\n",
    "                initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                name='sn_u',\n",
    "                trainable=False,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "        super(SpectralNormalization, self).build()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call `Layer`\"\"\"\n",
    "        # Recompute weights for each forward pass\n",
    "        self._compute_weights()\n",
    "        output = self.layer(inputs)\n",
    "        return output\n",
    "\n",
    "    def _compute_weights(self):\n",
    "        \"\"\"Generate normalized weights.\n",
    "        This method will update the value of self.layer.kernel with the\n",
    "        normalized value, so that the layer is ready for call().\n",
    "        \"\"\"\n",
    "        w_reshaped = tf.reshape(self.w, [-1, self.w_shape[-1]])\n",
    "        eps = 1e-12\n",
    "        _u = tf.identity(self.u)\n",
    "        _v = tf.matmul(_u, tf.transpose(w_reshaped))\n",
    "        _v = _v / tf.maximum(tf.reduce_sum(_v**2)**0.5, eps)\n",
    "        _u = tf.matmul(_v, w_reshaped)\n",
    "        _u = _u / tf.maximum(tf.reduce_sum(_u**2)**0.5, eps)\n",
    "\n",
    "        self.u.assign(_u)\n",
    "        sigma = tf.matmul(tf.matmul(_v, w_reshaped), tf.transpose(_u))\n",
    "\n",
    "        self.layer.kernel = self.w / sigma\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(\n",
    "            self.layer.compute_output_shape(input_shape).as_list())\n",
    "SN = SpectralNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnqvjQl3YgCs"
   },
   "outputs": [],
   "source": [
    "reg = tf.keras.regularizers.l2\n",
    "\n",
    "class CReLU(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(CReLU,self).__init__()\n",
    "    self.relu1 = tf.keras.layers.ReLU()\n",
    "    self.relu2 = tf.keras.layers.ReLU()\n",
    "    self.concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.concat([self.relu1(x), self.relu2(-x)])\n",
    "\n",
    "class MyModel(Model):\n",
    "  def __init__(self, n=32):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.d1 = Dense(n, name=\"d1\")\n",
    "    self.a1 = tf.keras.layers.ReLU()\n",
    "    self.d2 = Dense(n, name=\"d2\")\n",
    "    self.a2 = tf.keras.layers.ReLU()\n",
    "    self.out = Dense(2, activation='sigmoid', bias_initializer = tf.initializers.constant(value=0.0), name=\"out\")\n",
    "\n",
    "  def call(self, x, training=False):\n",
    "    x = tf.cast(x, tf.float32) * scale\n",
    "    x = self.d1(x)\n",
    "    x = self.a1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.a2(x)\n",
    "    return self.out(x)\n",
    "\n",
    "model = MyModel(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization, Loss and Metrics Definition and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mT9yjZmmb1xK"
   },
   "outputs": [],
   "source": [
    "clo = tf.keras.losses.BinaryCrossentropy()\n",
    "clo2 = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def custom_loss(pred_y, y):\n",
    "  return clo(pred_y, y) + clo2(tf.reduce_mean(pred_y, 1), tf.reduce_mean(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEPMSCZOY-5G"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxrsnnPEZbvc"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.MeanAbsoluteError(name='train_accuracy')\n",
    "mean_prediction = tf.keras.metrics.Mean(name='mean_prediction')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.MeanAbsoluteError(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7XxD8r49ces"
   },
   "outputs": [],
   "source": [
    "min_train_loss = tf.math.reduce_mean(loss_object(Y_train, Y_train))\n",
    "min_test_loss = tf.math.reduce_mean(loss_object(Y_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eydFDDDb-Hgh",
    "outputId": "9bc888e2-ce02-4799-a057-2df83389fb91"
   },
   "outputs": [],
   "source": [
    "min_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bc9yhe8ysXbI",
    "outputId": "094c8d71-1153-4c53-9691-507d95197b5d"
   },
   "outputs": [],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHp4Od2rp338"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def reset_metrics():\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  mean_prediction.reset_states()\n",
    "\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5GI25k6ZgzK"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images, training=True)\n",
    "    #regularization_loss = tf.math.add_n(model.losses)\n",
    "    pred_loss = loss_object(labels, predictions)\n",
    "    loss = pred_loss #+ regularization_loss\n",
    "    # loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "  mean_prediction(predictions)\n",
    "  train_loss(pred_loss)\n",
    "  train_accuracy(tf.reduce_mean(labels, 1), tf.reduce_mean(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkUM5jY_ZmJS"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(tf.reduce_mean(labels, 1), tf.reduce_mean(predictions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s_qiMoYCmimh",
    "outputId": "d75d8e4f-a6cd-4ac8-b76f-c8f0423c1af2"
   },
   "outputs": [],
   "source": [
    "for test_images, test_labels in test_ds:\n",
    "  test_step(test_images, test_labels)\n",
    "\n",
    "template = 'Initial test values are Test Loss: {}, Test Accuracy: {}'\n",
    "print (template.format(test_loss.result()-min_test_loss, test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 180\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  reset_metrics()\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  template = 'Epoch {0:}, Loss: {1:.5f}, Test Loss: {4:.5f}, Abs Loss: {2:.5f}, Test Abs Loss: {5:.5f}, Mean pred: {3:.5f}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result()-min_train_loss,\n",
    "                         train_accuracy.result(),\n",
    "                         mean_prediction.result(),\n",
    "                         test_loss.result()-min_test_loss,\n",
    "                         test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEIG1Fnw442F"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "6--3hut6A7Id",
    "outputId": "68c611f2-8d80-4fc5-b620-3847d9fbecb5"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ Source File Generation\n",
    "\n",
    "If you modify the model architecture then you will likely need to make changes here.\n",
    "\n",
    "This should generate the source you can use to replace the net_weights.h file in Winter's source directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Sn2yXdlRb8uD",
    "outputId": "143d985e-d1a0-457d-902e-a75ef10c8468",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"/*\")\n",
    "print(\" * net_weights.h\")\n",
    "print(\" *\")\n",
    "print(\" *  Created on: Jul 9, 2019\")\n",
    "print(\" *      Author: Jonathan\")\n",
    "print(\" */\")\n",
    "print(\"\")\n",
    "print(\"#ifndef SRC_NET_WEIGHTS_H_\")\n",
    "print(\"#define SRC_NET_WEIGHTS_H_\")\n",
    "print(\"\")\n",
    "print(\"#include <array>\")\n",
    "print(\"namespace net_hardcode {\")\n",
    "print(\"\")\n",
    "      \n",
    "weights = model.out.trainable_weights\n",
    "\n",
    "print(\"constexpr float bias_win = {};\".format(weights[1][0]))\n",
    "print(\"constexpr float bias_win_draw = {};\".format(weights[1][1]))\n",
    "print(\"\")\n",
    "\n",
    "print(\"constexpr std::array<float, {}> output_weights = {}\".format(weights[0].numpy().shape[0]\n",
    "                                                                    * weights[0].numpy().shape[1], \"{\"))\n",
    "for i in range(weights[0].numpy().shape[1]):\n",
    "  for j in range(weights[0].numpy().shape[0] // 4):\n",
    "    k = j*4\n",
    "    print(\"  {}, {}, {}, {},\".format(weights[0][k][i], weights[0][k+1][i],\n",
    "                                     weights[0][k+2][i], weights[0][k+3][i]))\n",
    "print(\"};\")\n",
    "print(\"\")\n",
    "\n",
    "weights = model.d2.trainable_weights\n",
    "\n",
    "print(\"constexpr std::array<float, {}> l2_bias = {}\".format(weights[1].numpy().shape[0], \"{\"))\n",
    "for j in range(weights[1].numpy().shape[0] // 4):\n",
    "  k = j*4\n",
    "  print(\"  {}, {}, {}, {},\".format(weights[1][k], weights[1][k+1],\n",
    "                                   weights[1][k+2], weights[1][k+3]))\n",
    "print(\"};\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"constexpr std::array<float, {}> l2_weights = {}\".format(weights[0].numpy().shape[0]\n",
    "                                                               * weights[0].numpy().shape[1], \"{\"))\n",
    "for i in range(weights[0].numpy().shape[0]):\n",
    "  for j in range(weights[0].numpy().shape[1] // 4):\n",
    "    k = j*4\n",
    "    print(\"  {}, {}, {}, {},\".format(weights[0][i][k], weights[0][i][k+1],\n",
    "                                     weights[0][i][k+2], weights[0][i][k+3]))\n",
    "print(\"};\")\n",
    "print(\"\")\n",
    "\n",
    "weights = model.d1.trainable_weights\n",
    "\n",
    "print(\"constexpr std::array<float, {}> l1_bias = {}\".format(weights[1].numpy().shape[0], \"{\"))\n",
    "for j in range(weights[1].numpy().shape[0] // 4):\n",
    "  k = j*4\n",
    "  print(\"  {}, {}, {}, {},\".format(weights[1][k], weights[1][k+1],\n",
    "                                   weights[1][k+2], weights[1][k+3]))\n",
    "print(\"};\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"constexpr std::array<float, {}> l1_weights = {}\".format(weights[0].numpy().shape[0]\n",
    "                                                               * weights[0].numpy().shape[1], \"{\"))\n",
    "for i in range(weights[0].numpy().shape[0]):\n",
    "  for j in range(weights[0].numpy().shape[1] // 4):\n",
    "    k = j*4\n",
    "    print(\"  {}, {}, {}, {},\".format(weights[0][i][k] * scale[i], weights[0][i][k+1] * scale[i],\n",
    "                                     weights[0][i][k+2] * scale[i], weights[0][i][k+3] * scale[i]))\n",
    "print(\"};\")\n",
    "print(\"\")\n",
    "print(\"}\")\n",
    "print(\"\")\n",
    "print(\"#endif /* SRC_NET_WEIGHTS_H_ */\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WinterNetIndPawnV3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
